{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ ドリフトの監視\n",
    "\n",
    "時間が経つにつれて、フィーチャ データの傾向の変化により、モデルの正確な予測の効果が低下する可能性があります。この現象は*データ ドリフト*と呼ばれ、必要に応じてモデルを再トレーニングできるように、機械学習ソリューションを監視して検出することが重要です。\n",
    "\n",
    "このラボでは、データセットのデータ ドリフト監視を構成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataDriftDetector モジュールをインストールする\n",
    "\n",
    "データ ドリフト モニターを定義するには、最新バージョンの Azure ML SDK がインストールされていることを確認し、**datadrift** モジュールをインストールして、以下の通り後続のセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade azureml-sdk[notebooks,automl,explain]\n",
    "!pip install --upgrade azureml-datadrift\n",
    "# インストールが完了したら、カーネルを再起動してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **重要**: <u>カーネルを再起動</u>する必要があります。Jupyter の **[カーネル]** メニューで、**[再起動と出力のクリア]** を選択します。次に、上のセルからの出力が削除され、カーネルが再起動されたら、以下の手順を続行します。\n",
    "\n",
    "## ワークスペースに接続する\n",
    "\n",
    "これで、Azure ML SDK を使用してワークスペースに接続する準備が整いました。\n",
    "\n",
    "> **注**: 前回の演習を完了してから Azure サブスクリプションとの認証済みセッションの有効期限が切れている場合は、再認証を求めるメッセージが表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# 保存した構成ファイルからワークスペースを読み込む\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to work with', ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベースライン データセットを作成する\n",
    "\n",
    "データ ドリフトのデータセットを監視するには、*ベースライン* データセット (通常、モデルのトレーニングに使用されるデータセット) を登録して、将来収集されるデータとの比較ポイントとして使用する必要があります。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "\n",
    "# ベースライン データをアップロードする\n",
    "default_ds = ws.get_default_datastore()\n",
    "default_ds.upload_files(files=['./data/diabetes.csv', './data/diabetes2.csv'],\n",
    "                       target_path='diabetes-baseline',\n",
    "                       overwrite=True, \n",
    "                       show_progress=True)\n",
    "\n",
    "# ベースライン データセットを作成して登録する\n",
    "print('Registering baseline dataset...')\n",
    "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-baseline/*.csv'))\n",
    "baseline_data_set = baseline_data_set.register(workspace=ws, \n",
    "                           name='diabetes baseline',\n",
    "                           description='diabetes baseline data',\n",
    "                           tags = {'format':'CSV'},\n",
    "                           create_new_version=True)\n",
    "\n",
    "print('Baseline dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ターゲット データセットを作成する\n",
    "\n",
    "時間の経過とともに、ベースライン トレーニング データと同じ機能を持つ新しいデータを収集できます。この新しいデータをベースライン データと比較するには、データ ドリフトを分析する機能を含むターゲット データセットと、新しいデータが最新であった時点を示すタイムスタンプ フィールドを定義する必要があります。これにより、一時的なサイクル間隔でのデータ ドリフトを測定します。タイムスタンプは、データセット自体のフィールド、またはデータの格納に使用されるフォルダーとファイル名パターンから派生したフィールドのいずれかです。たとえば、月のフォルダーを含む年のフォルダーと、その日のフォルダーを含むフォルダー階層に新しいデータを保存できます。または、次のようにファイル名に年、月、日をエンコードすることもできます。*data_2020-01-29.csv*。これは、次のコードで採用されているアプローチです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "print('Generating simulated data...')\n",
    "\n",
    "# 2 つのデータ ファイルのうち小さい方を読み込む\n",
    "data = pd.read_csv('data/diabetes2.csv')\n",
    "\n",
    "# 過去 6 週間のデータを生成します\n",
    "weeknos = reversed(range(6))\n",
    "\n",
    "file_paths = []\n",
    "for weekno in weeknos:\n",
    "    \n",
    "    # X 週間前の日付を取得する\n",
    "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n",
    "    \n",
    "    # データを変更してドリフトを作成する\n",
    "    data['Pregnancies'] = data['Pregnancies'] + 1\n",
    "    data['Age'] = round(data['Age'] * 1.2).astype(int)\n",
    "    data['BMI'] = data['BMI'] * 1.1\n",
    "    \n",
    "    # ファイル名にエンコードされた日付でファイルを保存する\n",
    "    file_path = 'data/diabetes_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n",
    "    data.to_csv(file_path)\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "# ファイルをアップロードする\n",
    "path_on_datastore = 'diabetes-target'\n",
    "default_ds.upload_files(files=file_paths,\n",
    "                       target_path=path_on_datastore,\n",
    "                       overwrite=True,\n",
    "                       show_progress=True)\n",
    "\n",
    "# フォルダー パーティション形式を使用して、タイムスタンプ列 'date' を持つデータセットを定義する\n",
    "partition_format = path_on_datastore + '/diabetes_{date:yyyy-MM-dd}.csv'\n",
    "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\n",
    "                                                       partition_format=partition_format)\n",
    "\n",
    "# ターゲット データセットを登録する\n",
    "print('Registering target dataset...')\n",
    "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n",
    "                                                                          name='diabetes target',\n",
    "                                                                          description='diabetes target data',\n",
    "                                                                          tags = {'format':'CSV'},\n",
    "                                                                          create_new_version=True)\n",
    "\n",
    "print('Target dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ ドリフト モニターを作成する\n",
    "\n",
    "これで、糖尿病データのデータ ドリフト モニターを作成する準備が整いました。データ ドリフト モニターは、定期的またはオンデマンドで実行され、ベースライン データセットとターゲット データセットを比較し、時間の経過とともに新しいデータが追加されます。\n",
    "\n",
    "### コンピューティング先を作成する\n",
    "\n",
    "データ ドリフト モニターを実行するには、コンピューティング先が必要です。このラボでは、前に作成したコンピューティング クラスターを使用します (存在しない場合は作成されます)。\n",
    "\n",
    "> **重要**: 実行する前に、*your-compute-cluster* を以下のコードのコンピューティング クラスターの名前に変更してください。クラスター名は、長さが 2 〜 16 文字のグローバルに一意の名前である必要があります。文字、数字、および - が有効です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"your-compute-cluster\"\n",
    "\n",
    "try:\n",
    "    # 既存のコンピューティング先を確認する\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # まだ存在しない場合は、作成します\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ ドリフト モニターを定義する\n",
    "\n",
    "これで、**DataDriftDetector** クラスを使用してデータのデータ ドリフト モニターを定義する準備ができました。データ ドリフトを監視する機能、監視プロセスの実行に使用するコンピューティング先の名前、データの比較頻度、アラートがトリガーされるデータのドリフトしきい値、データ収集を可能にする待機時間 (時間単位)を指定できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.datadrift import DataDriftDetector\n",
    "\n",
    "# フィーチャーリストを設定する\n",
    "features = ['Pregnancies', 'Age', 'BMI']\n",
    "\n",
    "# データ ドリフト検出機能を設定する\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'diabetes-drift-detector', baseline_data_set, target_data_set,\n",
    "                                                      compute_target=cluster_name, \n",
    "                                                      frequency='Week', \n",
    "                                                      feature_list=features, \n",
    "                                                      drift_threshold=.3, \n",
    "                                                      latency=24)\n",
    "monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モニターをバックフィルする\n",
    "\n",
    "6 週間のシミュレートされた毎週のデータ収集を含むベースライン データセットとターゲット データセットがあります。これを使用してモニターをバックフィルして、元のベースラインとターゲット データの間のデータ ドリフトを分析できます。\n",
    "\n",
    "> **注** バックフィル分析を実行するには、コンピューティング先を起動する必要があるため、実行に時間がかかる場合があります。ウィジェットは常に更新されて状態が表示されない場合があるため、リンクをクリックして、Azure Machine Learning Studio で実験の状態を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "backfill = monitor.backfill( dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
    "\n",
    "RunDetails(backfill).show()\n",
    "backfill.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ ドリフトを分析する\n",
    "\n",
    "次のコードを使用して、バックフィル実行で収集された時点のデータ ドリフトを調べることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_metrics = backfill.get_metrics()\n",
    "for metric in drift_metrics:\n",
    "    print(metric, drift_metrics[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次の手順に従って、[Azure Machine Learning Studio](https://ml.azure.com) でデータのドリフト メトリックを視覚化することもできます。\n",
    "\n",
    "1.**データセット** ページで、**データセット モニター** タブを表示します。\n",
    "2.表示するデータ ドリフト モニターをクリックします。\n",
    "3.データ ドリフト メトリックを表示する日付範囲を選択します (列グラフに複数の週のデータが表示されない場合は、1 分ほど待ってから**更新**をクリックします)。\n",
    "4.上部の**ドリフト概要**セクションのグラフを調べ、全体的なドリフトの大きさとフィーチャごとのドリフトの寄与度を表示します。\n",
    "5.下部の**機能の詳細**セクションのグラフを確認すると、個々のフィーチャのさまざまなドリフトの尺度を確認できます。\n",
    "\n",
    "> **注**: データのドリフト メトリックの理解については、Azure Machine Learning のドキュメントの「[データセットを監視する方法](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets#データ ドリフトの結果を理解する)」を参照してください。\n",
    "\n",
    "## さらに詳しく見る\n",
    "\n",
    "このラボは、データ ドリフト監視の概念と原則を紹介することを目的としています。データセットを使用したデータ ドリフトの監視の詳細については、Azure Machine Learning のドキュメントの[データセット上のデータ ドリフトの検出](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets)を参照してください。\n",
    "\n",
    "Azure Kubernetes Service (AKS) クラスターにデプロイされたサービスのデータ ドリフト監視を構成することもできます。この詳細については、Azure Machine Learning ドキュメントの「[Azure Kubernetes Service (AKS) にデプロイされたモデルでデータ ドリフトを検出する](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-data-drift)」を参照してください。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}